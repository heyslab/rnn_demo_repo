{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e761d85c-95a2-4ecf-a943-27f458eb0d5b",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks (in TensorFlow)\n",
    "- Also see tensorflow time series analysis tutorial: https://www.tensorflow.org/tutorials/structured_data/time_series\n",
    "- Simplified version of the RNNs used for the manuscript https://www.biorxiv.org/content/10.1101/2025.05.13.653873v1.full.pdf\n",
    "- Uses a cusomtized variant of SimpleRNN from tensorflow to include private noise term as well as a decay factor $\\gamma$\n",
    "\n",
    "$\\texttt{LeakyRNN}$ Equation:\n",
    "<p>\n",
    "$\\mathbf{h}_{t+1} = (1 - {\\gamma})\\mathbf{h}_t +{\\gamma}\\sigma\\left( \\mathbf{W}_{IN}\\mathbf{x}_t +\\mathbf{W}_{REC}\\mathbf{h}_t + \\mathbf{b} + \\mathbf{\\xi_t}\\right)$\n",
    "</p>\n",
    "\n",
    "<p>\n",
    "$y_t = \\mathbf{W}_{OUT}\\mathbf{h}_t$\n",
    "</p>\n",
    "\n",
    "<p style=\"text-align:left;float:left;margin-right:50px\">\n",
    "$$ \\begin{matrix*}[l]\n",
    " \\gamma &-& \\text{activity decay bias}\\\\\n",
    " \\mathbf{x}_t &-& \\text{input values} \\\\\n",
    " \\mathbf{W}_{IN} &-& \\text{input weights} \\\\\n",
    " \\mathbf{y}_t &-& \\text{output values} \\\\\n",
    " \\mathbf{b} &-& \\text{unit biases} \\\\\n",
    "\\end{matrix*} $$\n",
    "</p>\n",
    "<p style=\"text-align:left;float:left\">\n",
    "$$ \\begin{matrix*}[l]\n",
    " \\mathbf{\\xi}_t &-& \\text{private unit noise} \\\\\n",
    " \\mathbf{W}_{OUT} &-& \\text{output weights} \\\\\n",
    " \\mathbf{h}_t &-& \\text{unit activations} \\\\\n",
    " \\mathbf{W}_{REC} &-& \\text{connection weights} \\\\\n",
    " \\sigma &-& \\text{activation function (} \\mathtt{tanh}\\text{)} \\\\\n",
    "\\end{matrix*} $$\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376e429f-4b21-4559-b8b1-9d32a8fa8f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "\n",
    "# clone supporting files if running on colab, otherwise assume present\n",
    "if 'google.colab' in sys.modules:\n",
    "    os.chdir('/content')\n",
    "    if not Path('rnn_demo_repo').exists():\n",
    "      !git clone https://github.com/heyslab/rnn_demo_repo.git \n",
    "    os.chdir('/content/rnn_demo_repo')\n",
    "\n",
    "from classes.models import LeakyRNNCell, LeakyRNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f3a3c2-b4a0-4f38-84ce-ba385078eefb",
   "metadata": {},
   "source": [
    "### Define parameters for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e3d297-c47c-46a2-bacd-83f3d258f59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'units': 128,                       # number of units in recurrent layer\n",
    "    'activation': 'tanh',               # activation function for recurrent layer\n",
    "    'weights_regularizer_coef': 1e-3,   # L2 penalty on all weights\n",
    "    'activity_regularizer_coef': 1e-3,  # L2 penalty on activity\n",
    "    'learning_rate': 1e-5,\n",
    "    'noise_level': 0.3,                 # private noise on each unit\n",
    "    'input_noise': 0.15,                # noise on the inputs\n",
    "    'gamma': 0.2,                       # activity decay parameter\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122f1a5-47be-4898-b46e-d924855e6e06",
   "metadata": {},
   "source": [
    "### Setup and build the model\n",
    "\n",
    "Leaky RNN describes the inputs as well as the recurrent layer. [code here](classes/models.py)\n",
    "\n",
    "tf.keras.layers.Dense is the feed forward connections to the single output node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d713efa8-48f0-4b87-b877-f749b9f16b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "activity_regularizer = tf.keras.regularizers.L2(params['activity_regularizer_coef'])\n",
    "recurrent_regularizer = tf.keras.regularizers.L2(params['activity_regularizer_coef'])\n",
    "kernel_regularizer = tf.keras.regularizers.L2(params['weights_regularizer_coef'])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(params['learning_rate'])\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    LeakyRNN(params['units'], activation=params['activation'], gamma=params['gamma'],\n",
    "             return_sequences=True, activity_regularizer=activity_regularizer,\n",
    "             recurrent_regularizer=recurrent_regularizer, kernel_regularizer=kernel_regularizer),\n",
    "    tf.keras.layers.Dense(\n",
    "        units=1, activity_regularizer=activity_regularizer,\n",
    "        kernel_regularizer=kernel_regularizer)\n",
    "])\n",
    "model.compile(loss=tf.keras.losses.MeanSquaredError(),\n",
    "             optimizer=optimizer, metrics=[tf.keras.metrics.MeanSquaredError()])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e03562-005c-4866-ad92-b6c08ee39029",
   "metadata": {},
   "source": [
    "### Need a way to generate data for training the model\n",
    "- Here we generate data for tDNMS trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd316b0-7b36-4c2c-8b44-b389f65bc519",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trial_data(n_trials, input_noise):\n",
    "    \n",
    "    # generate np arrays with one-hot encoding of cue durations\n",
    "    trial_ls = np.zeros((200))\n",
    "    trial_ls[30:80] = 1\n",
    "    trial_ls[110:130] = 1\n",
    "\n",
    "    trial_sl = np.zeros((200))\n",
    "    trial_sl[30:50] = 1\n",
    "    trial_sl[80:130] = 1\n",
    "\n",
    "    trial_ss = np.zeros((200))\n",
    "    trial_ss[30:50] = 1\n",
    "    trial_ss[80:100] = 1\n",
    "\n",
    "    target_go = np.zeros(200)\n",
    "    target_go[131:160] = 1\n",
    "    target_nogo = np.zeros(200)\n",
    "\n",
    "    trial_key = {\n",
    "        'LS': trial_ls,\n",
    "        'SL': trial_sl,\n",
    "        'SS': trial_ss\n",
    "    }\n",
    "\n",
    "    target_key = {\n",
    "        'LS': target_go,\n",
    "        'SL': target_go,\n",
    "        'SS': target_nogo\n",
    "    }\n",
    "\n",
    "    # Also include a start cue as in standard tDNMS training\n",
    "    start_cue = np.zeros((200))\n",
    "    start_cue[0:3] = 1\n",
    "\n",
    "    # Here we omit the SS trial type to simplify trianing\n",
    "    block_trials = ['SL', 'LS']\n",
    "    \n",
    "    blocks = pd.DataFrame(np.tile(block_trials, (n_trials, 1)))\n",
    "    blocks.apply(random.shuffle, axis=1)\n",
    "    blocks = blocks.stack().reset_index(drop=True).reset_index()\n",
    "    blocks.columns = ['trial', 'trial_type']\n",
    "    blocks.index = pd.MultiIndex.from_frame(blocks)\n",
    "\n",
    "    trials = blocks['trial_type'].apply(lambda x, trial_key=trial_key: trial_key[x])\\\n",
    "                                .apply(pd.Series)  \n",
    "    trials.columns.name = 'time_bin'\n",
    "\n",
    "    start_cues = blocks['trial_type'].apply(lambda _, start_cue=start_cue: start_cue)\\\n",
    "                                    .apply(pd.Series)\n",
    "    start_cues.columns.name = 'time_bin'\n",
    "\n",
    "    targets =  blocks['trial_type'].apply(lambda x, target_key=target_key: target_key[x])\\\n",
    "                                  .apply(pd.Series)\n",
    "    targets.columns.name = 'time_bin'\n",
    "    trials = trials.stack()\n",
    "    targets = targets.stack()\n",
    "    start_cues = start_cues.stack()\n",
    "\n",
    "    trials = trials + np.random.normal(0, input_noise, trials.shape)\n",
    "    start_cues = start_cues + np.random.normal(0, input_noise, start_cues.shape)\n",
    "\n",
    "    return pd.concat((start_cues, trials), axis=1, keys=['light', 'odor']), targets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096f131d-bf4f-4257-a4bc-bd7fcee9c3a6",
   "metadata": {},
   "source": [
    "### Tensor flow requres data to be propery fomatted\n",
    "- Dimensions need to be batch_size x observations x features\n",
    "- here we use batch_size = 1, batches are trained simultenously and can speed up learning substantially"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716990f6-4bc8-4f98-ad18-4e82c2e44a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_for_training(X):\n",
    "    if len(X.shape) == 1:\n",
    "        return np.expand_dims(np.expand_dims(np.array(X.values), 0), -1)\n",
    "    elif len(X.shape) == 2:\n",
    "        return np.expand_dims(np.array(X.values), 0)\n",
    "\n",
    "    raise Exception('Can\\'t format')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f884f9-47ea-4b4f-9458-18873c14ef51",
   "metadata": {},
   "source": [
    "### Generate data for validation\n",
    "- validation data won't be trained on, but will be tested on each epoc and give a score to help us track model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae62cc-0a43-4956-8d59-1161c248cac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_trials, validation_targets = generate_trial_data(4, params['input_noise'])\n",
    "val_trials_fmt = format_for_training(validation_trials)\n",
    "val_target_fmt = format_for_training(validation_targets)\n",
    "history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6624ee-d9c7-4960-b837-944695d71b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    trials, targets = generate_trial_data(1, params['input_noise'])\n",
    "    targets_fmt = format_for_training(targets)\n",
    "    trials_fmt = format_for_training(trials)\n",
    "    history.append(model.fit(trials_fmt, targets_fmt, validation_data=(val_trials_fmt, val_target_fmt), epochs=1, batch_size=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fd5dde-df9f-4dfd-b39d-d7ebc72af32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse = list(map(lambda x: x.history['val_mean_squared_error'], history))\n",
    "plt.plot(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8603883-dd22-47e8-b02d-bb15e4cbe0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.layers[0](val_trials_fmt)\n",
    "o = model.layers[1](h)\n",
    "\n",
    "output = pd.Series(o[0, :, 0], index=validation_trials.index)\n",
    "trial_cues_series = (validation_trials['odor'] > 0.6).reset_index(drop=True)\n",
    "plt.plot(output.values)\n",
    "plt.plot(validation_targets.values)\n",
    "cue_times = pd.concat(\n",
    "    (trial_cues_series.where(trial_cues_series.astype(int).diff() > 0).dropna().reset_index()['index'],\n",
    "     trial_cues_series.where(trial_cues_series.astype(int).diff() < 0).dropna().reset_index()['index']),\n",
    "    keys=('starts',  'stops'), axis=1)\n",
    "cue_times.apply(lambda x: plt.gca().axvspan(*x, color='k', alpha=0.15), axis=1)\n",
    "plt.gca().spines['right'].set_visible(False)\n",
    "plt.gca().spines['top'].set_visible(False)\n",
    "plt.gca().set_xlim(0, 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1090a6-e6cd-4a73-abe0-213d53d0e39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "trials, _ = generate_trial_data(15, params['input_noise'])\n",
    "h = model.layers[0](format_for_training(trials))\n",
    "o = model.layers[1](h)\n",
    "\n",
    "output = pd.Series(o[0, :, 0], index=trials.index)\n",
    "\n",
    "h_df = pd.DataFrame(h[0], index=trials.index)\n",
    "pca = pd.DataFrame(PCA(n_components=3).fit_transform(h_df), h_df.index)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "gs = gridspec.GridSpec(1, 2, wspace=0.4)\n",
    "axs = list(map(plt.subplot, gs))\n",
    "pca[[0, 1]].groupby('trial_type').apply(lambda x, ax=axs[0]: ax.plot(*x.values.T))\n",
    "axs[0].set_ylabel('PC2')\n",
    "axs[0].set_xlabel('PC1')\n",
    "axs[0].set_aspect('equal')\n",
    "\n",
    "pca[[0, 2]].groupby('trial_type').apply(lambda x, ax=axs[1]: ax.plot(*x.values.T))\n",
    "axs[1].set_ylabel('PC3')\n",
    "axs[1].set_xlabel('PC1')\n",
    "axs[1].set_aspect('equal')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dec83e-f413-46ca-b95a-eaed48c84557",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "1. Use a Dataset Generator for the training data in order to generate unique data for each batch\n",
    "2. Identify Fixed and slow points:\n",
    "   - https://github.com/google-research/computation-thru-dynamics/blob/master/notebooks/Fixed%20Point%20Finder%20Tutorial.ipynb\n",
    "   - https://www.theoj.org/joss-papers/joss.01003/10.21105.joss.01003.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "keras",
   "language": "python",
   "name": "keras"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
